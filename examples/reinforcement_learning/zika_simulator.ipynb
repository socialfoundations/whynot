{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal control of intervention strategies for a Zika virus model\n",
    "\n",
    "This notebook demonstrates how to use WhyNot for sequential decision making experiments in a model of [Zika virus transmission](https://whynot-docs.readthedocs-hosted.com/en/latest/simulators.html#zika-simulator). The simulator is based on [the following paper](https://www.sciencedirect.com/science/article/pii/S2211692316301084#b25):\n",
    "\n",
    "- Momoh, Abdulfatai A., and Armin FÃ¼genschuh. \"Optimal control of intervention strategies and cost effectiveness analysis for a Zika virus model.\" Operations\n",
    "Research for Health Care 18 (2018): 99-111.\n",
    "\n",
    "We compare the efficacy of two policies at controlling the spread of Zika: a policy which does nothing and a constant treatment policy. Can you find something better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import whynot as wn\n",
    "import whynot.gym as gym\n",
    "\n",
    "from scripts import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "WhyNot provides exactly the same interface as the OpenAI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Zika-v0')\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct baseline policies\n",
    "\n",
    "We compare two trivial baselines. One policy that takes actions randomly and other that always opts\n",
    "to perform a mixed treatment strategy of $u_1, \\dots, u_4 = 0.5$ for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTreatmentPolicy():\n",
    "    def sample_action(self, obs):\n",
    "        return np.zeros(env.action_space.shape)\n",
    "\n",
    "class ConstantPolicy():\n",
    "    def sample_action(self, obs):\n",
    "        return 0.5 * np.ones(env.action_space.shape)\n",
    "    \n",
    "policies = {\n",
    "    \"No Treatment\": NoTreatmentPolicy(),\n",
    "    \"Constant Treatment\": ConstantPolicy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling trajectories\n",
    "\n",
    "We sample trajectories according to a policy by sampling an action `ac`, taking the action via `env.step(ac)`,\n",
    "observing the next state and reward, and then repeating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_episode_length):\n",
    "    \"\"\"Sample a single trajectory, acting according to the specified policy.\"\"\"\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the most recent observation to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = policy.sample_action(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        # Take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # Record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # End the rollout if the rollout ended\n",
    "        # Note that the rollout can end due to done, or due to max_episode_length\n",
    "        if done or steps > max_episode_length:\n",
    "            rollout_done = 1\n",
    "        else:\n",
    "            rollout_done = 0\n",
    "        terminals.append(rollout_done)\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sampled trajectories for each policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_trajectory(policies):\n",
    "    \"\"\"Plot sample trajectories from policies.\"\"\"\n",
    "    obs_dim_names = wn.zika.State.variable_names()\n",
    "\n",
    "    fig, axes = plt.subplots(5, 3, sharex=True, figsize=[18, 15])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        trajectory = sample_trajectory(env, policy, 400)\n",
    "        obs = trajectory[\"observation\"]\n",
    "        # Plot state evolution\n",
    "        for i in range(len(obs_dim_names)):\n",
    "            y = obs[:, i]\n",
    "            axes[i].plot(y, label=name)\n",
    "            axes[i].set_ylabel(obs_dim_names[i])\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "            axes[i].set_ylim(np.minimum(ymin, y.min()), np.maximum(ymax, y.max()))\n",
    "        \n",
    "        # Plot actions\n",
    "        \n",
    "        actions = np.array(trajectory[\"action\"])\n",
    "        for idx, label in enumerate([\"Bednet Use\", \"Condom Use\", \"Patient Treatment\", \"Indoor Spray Use\"]):\n",
    "            ax_idx = -5 + idx\n",
    "            axes[ax_idx].plot(actions[:, idx], label=name)\n",
    "            axes[ax_idx].set_ylabel(label)\n",
    "            axes[ax_idx].set_ylim(0.0, 1.0)\n",
    "        \n",
    "        # Plot reward\n",
    "        reward = trajectory[\"reward\"]\n",
    "        axes[-1].plot(reward, label=name)\n",
    "        axes[-1].set_ylabel(\"reward\")\n",
    "        axes[-1].ticklabel_format(scilimits=(-2, 2))\n",
    "        ymin, ymax = axes[-1].get_ylim()\n",
    "        axes[-1].set_ylim(np.minimum(ymin, reward.min()), np.maximum(ymax, reward.max()))\n",
    "    \n",
    "        print(f\"Total reward for {name}: {np.sum(reward):.2f}\")\n",
    "        \n",
    "    for ax in axes:\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Year\")\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(policies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
